<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tiny Slot-BERT | Rishabh Tole</title>
    <link rel="icon" type="image/png" href="../favicon.png">
    <meta name="description" content="Self-supervised object-centric video understanding on consumer hardware.">
    <link rel="stylesheet" href="../styles.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;700&family=Inter:wght@400;500;600&display=swap"
        rel="stylesheet">

    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar">
        <div class="container nav-container">
            <a href="../index.html" class="logo">RT.</a>
            <ul class="nav-links">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../experience.html">Work Experience</a></li>
                <li><a href="../projects.html">Projects</a></li>
                <li><a href="../service.html">Service</a></li>
                <li><a href="../music.html">Music</a></li>
                <li><a href="../contact.html">Contact</a></li>
                <li><a href="../Rishabh_Resume.pdf" class="btn btn-sm btn-outline">Resume</a></li>
            </ul>
        </div>
    </nav>

    <!-- Project Header -->
    <div class="page-header">
        <div class="container fade-in-up">
            <h1 style="margin-bottom: 20px;">Tiny Slot-BERT</h1>
            <p class="tagline" style="text-align:center; color: var(--text-secondary); margin-bottom: 30px;">
                Self-Supervised Object Video Segmentation
            </p>
            <div class="tech-stack" style="justify-content: center; margin-bottom: 30px; flex-wrap: wrap;">
                <span class="tech-tag">PyTorch</span>
                <span class="tech-tag">Self-Supervised Learning</span>
                <span class="tech-tag">Computer Vision</span>
                <span class="tech-tag">Transformers</span>
                <span class="tech-tag">Slot Attention</span>
            </div>
            <p style="text-align: center; max-width: 700px; margin: 0 auto; color: var(--text-secondary);">
                Reproducing and extending the Slot-BERT architecture for self-supervised object-centric video
                understanding on consumer hardware.
            </p>
            <div style="text-align: center; margin-top: 30px;">
                <a href="https://github.com/rishabh-tole/tiny-slot-bert" target="_blank" class="btn btn-primary">
                    View on GitHub →
                </a>
            </div>
        </div>
    </div>

    <!-- Case Study Content -->
    <section class="container" style="padding-top: 0;">
        <div class="glass-card fade-in-up visible" style="padding: 40px; text-align: left; align-items: flex-start;">


            <!-- Project Description Box -->
            <div class="project-intro" style="
                background: linear-gradient(145deg, rgba(255,255,255,0.05), rgba(255,255,255,0.02));
                border-radius: 12px;
                padding: 30px;
                margin-bottom: 50px;
                border: 1px solid rgba(255,255,255,0.1);
                box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            ">
                <div style="margin-bottom: 30px;">
                    <h3
                        style="color: var(--accent-primary); margin-bottom: 12px; font-size: 0.85rem; text-transform: uppercase; letter-spacing: 1.5px; font-weight: 600;">
                        The Goal</h3>
                    <p style="font-size: 1.15rem; line-height: 1.6; font-weight: 500;">
                        To build a self-supervised object-centric video model from scratch on consumer hardware, capable
                        of decomposing video into meaningful "object slots" without any labels.
                    </p>
                </div>

                <div
                    style="display: grid; grid-template-columns: 1fr 1fr; gap: 40px; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 25px;">
                    <div>
                        <h4
                            style="color: #000000; margin-bottom: 10px; font-size: 1rem; display: flex; align-items: center; gap: 8px;">
                            Inspiration
                        </h4>
                        <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6;">
                            I discovered the <a href="https://arxiv.org/abs/2501.12477" target="_blank"
                                style="color: #4da6ff; text-decoration: underline;">Slot-BERT
                                paper</a> (ICLR 2025) and was fascinated by the potential of learning object identities
                            purely through reconstruction, similar to how LLMs learn language.
                        </p>
                    </div>
                    <div>
                        <h4
                            style="color: #000000; margin-bottom: 10px; font-size: 1rem; display: flex; align-items: center; gap: 8px;">
                            As a Learner
                        </h4>
                        <p style="color: var(--text-secondary); font-size: 0.95rem; line-height: 1.6;">
                            The original paper used a massive model. My challenge is to reproduce these results and then
                            extend upon its ideas on my home PC (RTX 2070 Super) and optimize the architecture to
                            eventually run in real-time
                            on a webcam. Why you may ask? Cuz its cool!
                        </p>
                    </div>
                </div>
            </div>

            <!-- Demos & Discussion -->
            <section id="v1-demo" style="margin-bottom: 60px;">
                <h2>v1 Demo</h2>

                <p style="margin-bottom: 30px;">
                    This is the most basic version of the model. The architecture details are provided below.
                    <br><br>
                    Since we are not yet using any temporal reasoning (the Temporal Transformer is disabled), we
                    recalculate everything as if it is single-shot for each frame. This results in the "flashing"
                    segmentation you see below, as the slots do not stay consistent across frames (the "blue" slot in
                    frame 1 might be the "red" slot in frame 2).
                </p>

                <!-- Vertical Stacked Demos -->
                <div
                    style="display: flex; flex-direction: column; gap: 40px; align-items: center; margin-bottom: 50px;">
                    <div style="width: 100%; max-width: 800px;">
                        <img src="../images/slotbert/video_0_combined.gif" alt="Demo Video 0"
                            style="width: 100%; border-radius: 8px; box-shadow: 0 4px 15px rgba(0,0,0,0.3);">
                    </div>
                    <div style="width: 100%; max-width: 800px;">
                        <img src="../images/slotbert/video_1_combined.gif" alt="Demo Video 1"
                            style="width: 100%; border-radius: 8px; box-shadow: 0 4px 15px rgba(0,0,0,0.3);">
                    </div>
                    <div style="width: 100%; max-width: 800px;">
                        <img src="../images/slotbert/video_2_combined.gif" alt="Demo Video 2"
                            style="width: 100%; border-radius: 8px; box-shadow: 0 4px 15px rgba(0,0,0,0.3);">
                    </div>
                </div>

                <!-- Loss Curve -->
                <div style="text-align: center; margin-top: 50px;">
                    <h3>Training Loss</h3>
                    <img src="../images/slotbert/loss_graphs.png" alt="Training Loss Graphs"
                        style="max-width: 100%; border-radius: 8px; border: 1px solid rgba(255,255,255,0.1); margin-top: 15px;">
                    <p style="text-align: center; color: var(--text-secondary); font-size: 0.9rem; margin-top: 10px;">
                        Training Loss Curves</p>
                </div>
            </section>

            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Architecture (v1) -->
            <section id="new-architecture" style="margin-bottom: 60px;">
                <h2>Architecture & Process</h2>

                <h3>1. High-Level Architecture</h3>
                <p><strong>Tiny Slot-BERT</strong> is an object-centric video understanding model designed to be
                    lightweight and efficient. It decomposes video frames into a set of discrete "slots," where each
                    slot represents an object or background element. These slots track objects over time and can be used
                    to reconstruct the original video, ensuring they capture meaningful visual information.</p>
                <p>The architecture consists of four main components:</p>
                <ol style="padding-left: 25px; margin-top: 10px; line-height: 1.6;">
                    <li><strong>Visual Encoder</strong>: Extracts feature maps from input frames.</li>
                    <li><strong>Slot Attention</strong>: Iteratively refines a set of initialized memory slots to bind
                        to specific objects in the features.</li>
                    <li><strong>Temporal Transformer</strong>: (Disabled in Phase 1) Processes slots across time to
                        handle dynamics.</li>
                    <li><strong>Slot Decoder</strong>: Reconstructs the image and segmentation masks from the slots.
                    </li>
                </ol>
                <blockquote
                    style="border-left: 4px solid var(--accent-primary); padding-left: 15px; margin: 20px 0; background: rgba(255,255,255,0.05); padding: 15px; border-radius: 0 8px 8px 0;">
                    <strong>Note:</strong> In this implementation (Phase 1), the <strong>Temporal Transformer is
                        disabled</strong>. The model processes each frame independently to first learn robust object
                    discovery (slot attention) and reconstruction before adding the complexity of cross-frame reasoning.
                    This staged approach makes training more stable.
                </blockquote>

                <h3>2. Component Details</h3>

                <h4>A. Simple Convolutional Encoder (<code>models/encoder.py</code>)</h4>
                <p>A lightweight CNN encoder is used for efficiency in Phase 1.</p>
                <ul style="padding-left: 25px; margin-top: 10px; line-height: 1.6;">
                    <li><strong>Structure</strong>: A stack of 4 convolutional blocks with BatchNorm and ReLU.</li>
                    <li><strong>Input</strong>: RGB Video Frames (\(B \times 3 \times H \times W\)).</li>
                    <li><strong>Downsampling</strong>: Reduces input spatial dimensions by <strong>16x</strong> (e.g.,
                        \(128 \times 128 \to 8 \times 8\)).</li>
                    <li><strong>Output</strong>: Feature map (\(B \times N \times D\)), where \(N = 64\) and \(D =
                        384\).</li>
                </ul>

                <h4>B. Slot Attention (<code>models/slot_attention.py</code>)</h4>
                <p>The core mechanism for object discovery.</p>
                <ul style="padding-left: 25px; margin-top: 10px; line-height: 1.6;">
                    <li><strong>Input</strong>: Features from the encoder + Positional Embeddings.</li>
                    <li><strong>Slots</strong>: A set of \(K\) learnable vectors (randomly initialized per frame with
                        high variance to prevent collapse).</li>
                    <li><strong>Process</strong>:
                        <ul style="padding-left: 20px; margin-top: 5px;">
                            <li><strong>Iterative Attention</strong>: Over \(T\) iterations (default 3), slots compete
                                to "explain" parts of the feature map via Softmax attention.</li>
                            <li><strong>Key/Query/Value</strong>: Slots act as Queries; Features act as Keys and Values.
                            </li>
                            <li><strong>Update</strong>: Slots are updated using a GRU (Gated Recurrent Unit) based on
                                the accumulated weighted values they attend to.</li>
                        </ul>
                    </li>
                    <li><strong>Output</strong>: Refined Slots (\(B \times K \times D_{slot}\)) and Attention Masks
                        (which serve as segmentation masks).</li>
                </ul>

                <h4>C. Slot Decoder (<code>models/decoder.py</code>)</h4>
                <p>Responsible for reconstructing the image from the abstract slots.</p>
                <ul style="padding-left: 25px; margin-top: 10px; line-height: 1.6;">
                    <li><strong>Type</strong>: <strong>Spatial Broadcast Decoder</strong>.</li>
                    <li><strong>Process</strong>:
                        <ol style="padding-left: 20px; margin-top: 5px;">
                            <li><strong>Broadcast</strong>: Each slot is replicated across a spatial grid.</li>
                            <li><strong>Positional Encoding</strong>: Learnable 2D positional embeddings are added.</li>
                            <li><strong>Pixel Decoder</strong>: A small CNN processes grids to produce RGB + Mask.</li>
                            <li><strong>Composition</strong>: Final image is a weighted sum of RGB sprites using Masks.
                            </li>
                        </ol>
                    </li>
                    <li><strong>Key Feature</strong>: The <strong>Slot Attention Masks</strong> are passed directly to
                        reconstruction to force the attention mechanism to learn accurate segmentation.</li>
                </ul>

                <h3>3. Training Losses</h3>
                <p>The model is trained with a combination of losses designed to encourage meaningful object
                    segmentation:</p>


                <div style="overflow-x: auto;">
                    <table
                        style="width: 100%; border-collapse: collapse; margin: 20px 0; background: rgba(255,255,255,0.03);">
                        <thead style="background: rgba(255,255,255,0.05);">
                            <tr>
                                <th
                                    style="padding: 10px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1);">
                                    Loss</th>
                                <th
                                    style="padding: 10px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1);">
                                    Description</th>
                                <th
                                    style="padding: 10px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1);">
                                    Weight</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                    <strong>Reconstruction (MSE)</strong>
                                </td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">Pixel-space
                                    L2 loss between input and reconstruction. The primary signal.</td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">1.0</td>
                            </tr>
                            <tr>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                    <strong>Orthogonality</strong>
                                </td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">Penalizes
                                    slot similarity (cosine) to encourage diverse representations.</td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">0.0
                                    (disabled)</td>
                            </tr>
                            <tr>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);"><strong>Mask
                                        Entropy</strong></td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">Encourages
                                    sharp, decisive segmentation masks (low per-pixel entropy).</td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">0.0
                                    (disabled)</td>
                            </tr>
                            <tr>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);"><strong>Slot
                                        Coverage</strong></td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">Prevents
                                    "dead" slots that cover no area. Uses exponential penalty + entropy bonus.</td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">0.0
                                    (disabled)</td>
                            </tr>
                            <tr>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                    <strong>Appearance Consistency</strong>
                                </td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                    <strong>Critical.</strong> Penalizes color variance within a slot's mask. Forces
                                    slots to align with object boundaries.
                                </td>
                                <td style="padding: 10px; border-bottom: 1px solid rgba(255,255,255,0.05);">0.0
                                    (disabled)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <blockquote
                    style="border-left: 4px solid var(--accent-primary); padding-left: 15px; margin: 20px 0; background: rgba(255,255,255,0.05); padding: 15px; border-radius: 0 8px 8px 0;">
                    <strong>Note on Disabled Losses:</strong> In certain training configurations, auxiliary losses
                    (ortho, entropy, coverage, appearance) are set to 0 to initially let the model learn reconstruction
                    freely. These can be progressively enabled to refine slot specialization.
                </blockquote>

                <h3>4. Inference Process</h3>
                <ol style="padding-left: 25px; margin-top: 10px; line-height: 1.6;">
                    <li><strong>Input</strong>: A video clip (e.g., 24 frames of \(128 \times 128\)).</li>
                    <li><strong>Encoding</strong>: The encoder processes each frame independently to extract feature
                        maps. Fourier positional embeddings are added.</li>
                    <li><strong>Slot Extraction</strong>: Slot Attention runs on each frame's features to extract \(K\)
                        slots (e.g., 7 slots).</li>
                    <li><strong>Decoding</strong>:
                        <ul style="padding-left: 20px; margin-top: 5px;">
                            <li>Slots are passed to the Decoder.</li>
                            <li>The decoder produces an RGB reconstruction for each slot.</li>
                            <li>Per-slot images are combined into a full frame reconstruction.</li>
                        </ul>
                    </li>
                    <li><strong>Visualization</strong>:
                        <ul style="padding-left: 20px; margin-top: 5px;">
                            <li><strong>Original</strong>: Input video.</li>
                            <li><strong>Segmentation</strong>: An argmax visualization showing which slot "owns" each
                                pixel.</li>
                            <li><strong>Slots</strong>: Visualization of what individual slots "see" (original image
                                masked by slot attention).</li>
                            <li><strong>Reconstruction</strong>: Model's attempt to recreate the video.</li>
                        </ul>
                    </li>
                </ol>
            </section>



            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Project Goals -->
            <div id="goals" style="margin-bottom: 60px;">
                <h2>Project Goals</h2>
                <p style="margin-bottom: 20px;">Build a self-supervised video model that achieves the following:</p>

                <table
                    style="width: 100%; border-collapse: collapse; margin-top: 15px; background: rgba(255,255,255,0.03); border-radius: 8px; overflow: hidden;">
                    <thead>
                        <tr style="background: rgba(255,255,255,0.05);">
                            <th
                                style="padding: 15px 20px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1);">
                                Goal</th>
                            <th
                                style="padding: 15px 20px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1);">
                                Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                <strong>Object Slots</strong>
                            </td>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">Represent
                                each frame using a small set of learned object slots</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                <strong>Soft Segmentation</strong>
                            </td>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">Learn
                                segmentation masks without any labels</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                <strong>Temporal Identity</strong>
                            </td>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">Maintain
                                object identity across time</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">
                                <strong>Occlusion Handling</strong>
                            </td>
                            <td style="padding: 12px 20px; border-bottom: 1px solid rgba(255,255,255,0.05);">Handle
                                occlusion and object re-entry</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px 20px;"><strong>Lightweight</strong></td>
                            <td style="padding: 12px 20px;">Run on a single consumer GPU (RTX 2070 Super)</td>
                        </tr>
                    </tbody>
                </table>

                <div style="margin-top: 30px;">
                    <h3>Development Phases</h3>
                    <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 20px;">
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px; border-left: 3px solid #4CAF50;">
                            <h4 style="color: #4CAF50;">Phase 1</h4>
                            <p style="margin-top: 8px;"><strong>Reproduction</strong></p>
                            <p style="color: var(--text-secondary); font-size: 0.9rem;">Slot-BERT core architecture</p>
                        </div>
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px; border-left: 3px solid #2196F3;">
                            <h4 style="color: #2196F3;">Phase 2</h4>
                            <p style="margin-top: 8px;"><strong>Validation</strong></p>
                            <p style="color: var(--text-secondary); font-size: 0.9rem;">Object-centric benchmark (MOVi)
                            </p>
                        </div>
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px; border-left: 3px solid #9C27B0;">
                            <h4 style="color: #9C27B0;">Phase 3</h4>
                            <p style="margin-top: 8px;"><strong>Extension</strong></p>
                            <p style="color: var(--text-secondary); font-size: 0.9rem;">Online/streaming inference</p>
                        </div>
                    </div>
                </div>
            </div>

            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Problem Definition -->
            <div id="problem" style="margin-bottom: 60px;">
                <h2>Problem Definition</h2>

                <div style="display: flex; gap: 30px; flex-wrap: wrap; margin-top: 25px;">
                    <div
                        style="flex: 1; min-width: 280px; background: rgba(255,255,255,0.03); padding: 25px; border-radius: 12px;">
                        <h3 style="color: #4CAF50;">Input</h3>
                        <p style="margin-top: 15px;">Unlabeled RGB video clips:</p>
                        <p
                            style="font-size: 1.1rem; margin-top: 10px; text-align: center; padding: 15px; background: rgba(0,0,0,0.2); border-radius: 8px;">
                            \(\{I_1, I_2, \ldots, I_T\}\)
                        </p>
                    </div>

                    <div
                        style="flex: 1; min-width: 280px; background: rgba(255,255,255,0.03); padding: 25px; border-radius: 12px;">
                        <h3 style="color: #2196F3;">Output</h3>
                        <p style="margin-top: 15px;">For each frame \(t\):</p>
                        <ul
                            style="padding-left: 25px; color: var(--text-secondary); margin-top: 10px; line-height: 1.8;">
                            <li>\(K\) slot embeddings: \(S_{t,k} \in \mathbb{R}^D\)</li>
                            <li>Soft segmentation masks: \(M_{t,k} \in [0,1]^{H \times W}\)</li>
                        </ul>
                    </div>
                </div>

                <div
                    style="margin-top: 25px; background: rgba(255,100,100,0.1); padding: 25px; border-radius: 12px; border: 1px solid rgba(255,100,100,0.2);">
                    <h3 style="color: #ff6b6b;">Constraints (Self-Supervised = No Labels!)</h3>
                    <ul style="padding-left: 25px; color: var(--text-secondary); margin-top: 15px; line-height: 1.8;">
                        <li>No ground-truth segmentation</li>
                        <li>No object identity labels</li>
                        <li>No supervision beyond reconstruction</li>
                    </ul>
                </div>
            </div>

            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Architecture -->
            <div id="architecture" style="margin-bottom: 60px;">
                <h2>High-Level Architecture</h2>
                <p
                    style="background: rgba(255,255,255,0.05); padding: 15px 20px; border-radius: 8px; border-left: 3px solid var(--accent-primary); margin-bottom: 25px;">
                    <strong>Core Insight:</strong> Objects are represented as latent slots. Slots are treated as tokens.
                    Temporal reasoning operates over slots, not pixels.
                </p>

                <pre
                    style="background: rgba(0,0,0,0.3); padding: 25px; border-radius: 12px; overflow-x: auto; font-size: 0.85rem; line-height: 1.6; color: var(--text-secondary);">
Video frames
     ↓
┌─────────────────────────┐
│  Frozen Visual Encoder  │  (ViT / CNN, pretrained)
└─────────────────────────┘
     ↓
Patch-level feature maps
     ↓
┌─────────────────────────┐
│     Slot Attention      │  (per-frame object discovery)
└─────────────────────────┘
     ↓
Object slots (K per frame)
     ↓
┌─────────────────────────┐
│ Temporal Slot Transformer│  (BERT-style, across time)
└─────────────────────────┘
     ↓
┌─────────────────────────┐
│      Slot Decoder       │  (reconstructions + masks)
└─────────────────────────┘
     ↓
Feature reconstruction loss</pre>

                <div style="margin-top: 30px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                    <h4>Key Design Decisions</h4>
                    <ul style="padding-left: 25px; color: var(--text-secondary); margin-top: 15px; line-height: 1.8;">
                        <li><strong>Frozen encoder:</strong> Reduces memory footprint and training time significantly
                        </li>
                        <li><strong>Per-frame slots:</strong> Slot Attention operates independently per frame before
                            temporal modeling</li>
                        <li><strong>Feature-space reconstruction:</strong> More robust than pixel-space, captures
                            semantics</li>
                        <li><strong>Bidirectional attention:</strong> Enables reasoning about past and future context
                        </li>
                    </ul>
                </div>
            </div>

            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Core Components Deep Dive -->
            <div id="components" style="margin-bottom: 60px;">
                <h2>Core Components (Deep Dive)</h2>

                <!-- Visual Encoder -->
                <div style="margin-top: 30px; margin-bottom: 45px;">
                    <h3>1. Frozen Visual Encoder</h3>
                    <p style="margin-top: 15px;">
                        Converts images into spatial feature maps using a pretrained model. <strong>Frozen during
                            training</strong> to reduce compute and isolate object-centric learning.
                    </p>

                    <div
                        style="background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px; margin-top: 20px;">
                        <h4>Options Considered:</h4>
                        <table style="width: 100%; border-collapse: collapse; margin-top: 15px;">
                            <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
                                <td style="padding: 12px 0;"><strong>DINO ViT-S/8</strong></td>
                                <td style="padding: 12px 0; color: var(--text-secondary);">Best quality features, works
                                    great for object discovery</td>
                            </tr>
                            <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
                                <td style="padding: 12px 0;"><strong>ResNet-18</strong></td>
                                <td style="padding: 12px 0; color: var(--text-secondary);">Lightweight fallback, faster
                                    but lower quality</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px 0;"><strong>DINOv2</strong></td>
                                <td style="padding: 12px 0; color: var(--text-secondary);">Latest and greatest,
                                    exploring as an upgrade</td>
                            </tr>
                        </table>
                    </div>

                    <p style="margin-top: 20px; font-size: 0.9rem; color: var(--text-secondary);">
                        <strong>Output:</strong> For an input image of size \(H \times W\), produces feature maps of
                        shape
                        \((H/p) \times (W/p) \times D\) where \(p\) is the patch size and \(D\) is feature dimension.
                    </p>
                </div>

                <!-- Slot Attention -->
                <div style="margin-bottom: 45px;">
                    <h3>2. Slot Attention Module</h3>
                    <p style="margin-top: 15px;">
                        The heart of object discovery. Takes spatial features and produces \(K\) object slots per frame.
                        Each slot softly "claims" regions of the image through <strong>competitive attention</strong>.
                    </p>

                    <div
                        style="background: rgba(255,255,255,0.05); padding: 20px; border-radius: 12px; margin-top: 20px; border-left: 3px solid var(--accent-primary);">
                        <strong>Key Insight:</strong> Segmentation emerges because reconstruction is easiest when slots
                        specialize.
                        The model learns to assign different objects to different slots without any supervision!
                    </div>

                    <h4 style="margin-top: 25px;">Algorithm (Iterative Refinement):</h4>
                    <ol style="padding-left: 25px; color: var(--text-secondary); margin-top: 15px; line-height: 2;">
                        <li>Initialize \(K\) slots from a learned distribution: \(s_k \sim \mathcal{N}(\mu, \sigma)\)
                        </li>
                        <li>Compute attention weights between slots and spatial features</li>
                        <li>Normalize weights across slots (competition via softmax)</li>
                        <li>Update slots via weighted aggregation of features</li>
                        <li>Apply GRU update for stability</li>
                        <li>Repeat for \(T\) iterations (typically 3-5)</li>
                    </ol>

                    <div style="margin-top: 20px; padding: 20px; background: rgba(0,0,0,0.2); border-radius: 8px;">
                        <h4>Attention Computation:</h4>
                        <p style="margin-top: 10px; text-align: center; font-size: 1rem;">
                            \[\text{attn}_{ij} = \frac{\exp(q_i \cdot k_j / \sqrt{D})}{\sum_{i'} \exp(q_{i'} \cdot k_j /
                            \sqrt{D})}\]
                        </p>
                        <p
                            style="margin-top: 15px; color: var(--text-secondary); font-size: 0.9rem; text-align: center;">
                            Note: Softmax is over <em>slots</em>, not positions. This is what creates competition!
                        </p>
                    </div>
                </div>

                <!-- Temporal Slot Transformer -->
                <div style="margin-bottom: 45px;">
                    <h3>3. Temporal Slot Transformer (Slot-BERT)</h3>
                    <p style="margin-top: 15px;">
                        <span
                            style="background: linear-gradient(90deg, var(--accent-primary), #9C27B0); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: bold;">
                            This is the main contribution of the paper!
                        </span>
                    </p>
                    <p style="margin-top: 10px;">
                        Slots from all frames are treated as tokens in a bidirectional transformer.
                        This enables powerful temporal reasoning.
                    </p>

                    <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 25px;">
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                            <h4 style="color: #4CAF50;">✓ Identity Persistence</h4>
                            <p style="color: var(--text-secondary); margin-top: 10px; font-size: 0.9rem;">
                                Objects maintain consistent slot assignments across frames
                            </p>
                        </div>
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                            <h4 style="color: #2196F3;">✓ Occlusion Recovery</h4>
                            <p style="color: var(--text-secondary); margin-top: 10px; font-size: 0.9rem;">
                                Can "remember" objects that temporarily leave the frame
                            </p>
                        </div>
                        <div
                            style="flex: 1; min-width: 200px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                            <h4 style="color: #9C27B0;">✓ Long-Range Context</h4>
                            <p style="color: var(--text-secondary); margin-top: 10px; font-size: 0.9rem;">
                                Bidirectional attention sees both past and future
                            </p>
                        </div>
                    </div>

                    <div
                        style="margin-top: 25px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                        <h4>Token Structure:</h4>
                        <p style="margin-top: 10px; color: var(--text-secondary);">
                            For a video with \(T\) frames and \(K\) slots per frame, the transformer receives \(T \times
                            K\) tokens.
                            Each token is the concatenation of:
                        </p>
                        <ul style="padding-left: 25px; color: var(--text-secondary); margin-top: 10px;">
                            <li>Slot embedding from Slot Attention</li>
                            <li>Learned temporal position encoding</li>
                            <li>Learned slot index encoding</li>
                        </ul>
                    </div>
                </div>

                <!-- Slot Decoder -->
                <div style="margin-bottom: 30px;">
                    <h3>4. Slot Decoder</h3>
                    <p style="margin-top: 15px;">
                        Each slot is decoded independently to produce reconstructions and masks.
                    </p>

                    <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 20px;">
                        <div
                            style="flex: 1; min-width: 250px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                            <h4>Per-Slot Output:</h4>
                            <ul
                                style="padding-left: 20px; color: var(--text-secondary); margin-top: 10px; line-height: 1.8;">
                                <li>Feature contribution: \(\hat{f}_k \in \mathbb{R}^{H \times W \times D}\)</li>
                                <li>Spatial mask (logits): \(\alpha_k \in \mathbb{R}^{H \times W}\)</li>
                            </ul>
                        </div>
                        <div
                            style="flex: 1; min-width: 250px; background: rgba(255,255,255,0.03); padding: 20px; border-radius: 12px;">
                            <h4>Composition:</h4>
                            <p style="margin-top: 10px; text-align: center;">
                                \[\hat{f} = \sum_k m_k \odot \hat{f}_k\]
                            </p>
                            <p style="margin-top: 10px; text-align: center;">
                                \[m_k = \text{softmax}_k(\alpha_k)\]
                            </p>
                        </div>
                    </div>

                    <p
                        style="margin-top: 20px; padding: 15px 20px; background: rgba(255,255,255,0.05); border-radius: 8px; border-left: 3px solid #4CAF50;">
                        <strong>Important:</strong> The model reconstructs <em>features</em>, not pixels.
                        This is more robust and captures semantic content rather than low-level texture.
                    </p>
                </div>
            </div>

            <hr style="border-top: 1px solid rgba(255,255,255,0.1); margin: 40px 0;">

            <!-- Training Objective -->
            <div id="training" style="margin-bottom: 60px;">
                <h2>Training Objective</h2>

                <!-- Masked Slot Modeling -->
                <div style="margin-top: 25px; margin-bottom: 35px;">
                    <h3>Masked Slot Modeling (MSM)</h3>
                    <p style="margin-top: 15px;">
                        Analogous to masked language modeling in BERT, but for visual slots!
                    </p>

                    <div style="display: flex; gap: 15px; margin-top: 20px; flex-wrap: wrap;">
                        <div
                            style="flex: 1; min-width: 180px; background: rgba(255,255,255,0.03); padding: 15px 20px; border-radius: 8px; text-align: center;">
                            <div style="font-size: 1.5rem; margin-bottom: 8px;">1️⃣</div>
                            <p style="font-size: 0.9rem;">Randomly mask slot tokens across time</p>
                        </div>
                        <div
                            style="flex: 1; min-width: 180px; background: rgba(255,255,255,0.03); padding: 15px 20px; border-radius: 8px; text-align: center;">
                            <div style="font-size: 1.5rem; margin-bottom: 8px;">2️⃣</div>
                            <p style="font-size: 0.9rem;">Transformer infers missing slots from context</p>
                        </div>
                        <div
                            style="flex: 1; min-width: 180px; background: rgba(255,255,255,0.03); padding: 15px 20px; border-radius: 8px; text-align: center;">
                            <div style="font-size: 1.5rem; margin-bottom: 8px;">3️⃣</div>
                            <p style="font-size: 0.9rem;">Compute reconstruction loss in feature space</p>
                        </div>
                    </div>

                    <div style="margin-top: 20px; padding: 20px; background: rgba(0,0,0,0.2); border-radius: 8px;">
                        <p style="text-align: center; font-size: 1rem;">
                            \[\mathcal{L}_{\text{reconstruction}} = \sum_{t,i,j} \| f_{t,i,j} - \hat{f}_{t,i,j} \|^2\]
                        </p>
                    </div>
                </div>

                <!-- Slot Orthogonality -->
                <div style="margin-bottom: 35px;">
                    <h3>Slot Orthogonality Regularization</h3>
                    <p style="margin-top: 15px;">
                        Penalizes redundant slots to encourage <strong>disentangled object representations</strong>
                        and prevent slot collapse (where all slots learn the same thing).
                    </p>

                    <div style="margin-top: 20px; padding: 20px; background: rgba(0,0,0,0.2); border-radius: 8px;">
                        <p style="text-align: center; font-size: 1rem;">
                            \[\mathcal{L}_{\text{orthogonality}} = \sum_{i \neq j} |s_i \cdot s_j|^2\]
                        </p>
                    </div>
                </div>

                <!-- Total Loss -->
                <div
                    style="background: linear-gradient(135deg, rgba(255,255,255,0.05), rgba(255,255,255,0.02)); padding: 30px; border-radius: 12px; border: 1px solid rgba(255,255,255,0.1);">
                    <h3 style="text-align: center;">Total Loss</h3>
                    <p
                        style="font-size: 1.2rem; margin-top: 20px; text-align: center; padding: 20px; background: rgba(0,0,0,0.3); border-radius: 8px;">
                        \[\mathcal{L} = \mathcal{L}_{\text{reconstruction}} + \lambda
                        \mathcal{L}_{\text{orthogonality}}\]
                    </p>
                    <p style="text-align: center; margin-top: 15px; color: var(--text-secondary);">
                        where \(\lambda\) is a hyperparameter (typically 0.1-1.0)
                    </p>
                </div>
            </div>

            <div style="margin-top: 50px; text-align: center; width: 100%;">
                <a href="https://github.com/rishabh-tole/tiny-slot-bert" target="_blank" class="btn btn-primary">
                    View on GitHub →
                </a>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 Rishabh Tole.</p>
        </div>
    </footer>

    <script src="../script.js"></script>
</body>

</html>